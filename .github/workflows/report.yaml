name: Report - Weekly Testbed Report

on:
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: "0 2 * * 0"
  workflow_dispatch:
    inputs:
      report_format:
        description: 'Report format (html, pdf, both)'
        required: false
        default: 'both'
        type: choice
        options:
          - html
          - pdf
          - both
      time_range:
        description: 'Time range in hours'
        required: false
        default: '168'
        type: string
  push:
    branches: [main]
    paths:
      - "testbed/tools/reporter/**"
      - "testbed/runtime/**"
      - "testbed/metrics/**"

env:
  PYTHON_VERSION: "3.11"
  REPORT_OUTPUT_DIR: "./reports"
  ART_RESULTS_PATH: "./art-results/latest.json"
  REDTEAM_RESULTS_PATH: "./redteam-results/latest.json"
  GITHUB_PAGES_BRANCH: "gh-pages"

jobs:
  generate-report:
    name: Generate Testbed Report
    runs-on: ubuntu-latest

    services:
      prometheus:
        image: prom/prometheus:latest
        ports:
          - 9090:9090
        volumes:
          - ./testbed/ops/prometheus:/etc/prometheus
          - prometheus_data:/prometheus
        options: >-
          --config.file=/etc/prometheus/prometheus.yml
          --storage.tsdb.path=/prometheus
          --web.console.libraries=/etc/prometheus/console_libraries
          --web.console.templates=/etc/prometheus/consoles
          --storage.tsdb.retention.time=200h
          --web.enable-lifecycle

      ledger:
        image: node:18-alpine
        ports:
          - 8080:8080
        volumes:
          - ./testbed/runtime/ledger:/app
        options: >-
          sh -c "
            npm install &&
            npm start
          "

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            libffi-dev \
            libssl-dev \
            pkg-config \
            wkhtmltopdf \
            fonts-liberation \
            libfontconfig1

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            aiohttp \
            jinja2 \
            pyyaml \
            pandas \
            matplotlib \
            seaborn \
            weasyprint \
            plotly \
            kaleido \
            prometheus-api-client \
            kubernetes \
            kubernetes-client

      - name: Create configuration
        run: |
          mkdir -p config
          cat > config/report-config.yaml << EOF
          prometheus_url: http://localhost:9090
          ledger_url: http://localhost:8080
          art_results_path: ${{ env.ART_RESULTS_PATH }}
          redteam_results_path: ${{ env.REDTEAM_RESULTS_PATH }}
          include_art_comparison: true
          include_redteam_analysis: true
          include_cost_analysis: true
          include_performance_trends: true
          kpi_thresholds:
            latency_p95: 2.0
            latency_p99: 5.0
            block_rate: 0.01
            cross_tenant_interactions: 0
            data_leaks: 0
            cost_per_1k_transactions: 0.05
            confidence_score: 0.95
            fallback_rate: 0.05
            availability: 0.999
            throughput_min: 100
          report_sections:
            - executive_summary
            - performance_metrics
            - security_metrics
            - cost_analysis
            - reliability_metrics
            - art_comparison
            - redteam_analysis
            - recommendations
            - appendix
          output_formats:
            - html
            - pdf
            - json
            - markdown
          EOF

      - name: Wait for services
        run: |
          echo "Waiting for Prometheus to be ready..."
          timeout 60 bash -c 'until curl -s http://localhost:9090/-/ready; do sleep 2; done'

          echo "Waiting for Ledger to be ready..."
          timeout 60 bash -c 'until curl -s http://localhost:8080/health; do sleep 2; done'

      - name: Generate test data
        run: |
          # Create sample ART results for testing
          mkdir -p art-results
          cat > art-results/latest.json << EOF
          {
            "latency_p95": 1.8,
            "latency_p99": 4.5,
            "throughput": 120.0,
            "error_rate": 0.02,
            "block_rate": 0.008,
            "cost_per_1k": 0.042,
            "confidence_score": 0.97,
            "fallback_rate": 0.03,
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

          # Create sample red-team results
          mkdir -p redteam-results
          cat > redteam-results/latest.json << EOF
          [
            {
              "test_name": "injection_test",
              "status": "pass",
              "last_run": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "failure_rate": 0.0,
              "severity": "high",
              "details": "SQL injection prevention working correctly",
              "run_url": "https://github.com/org/repo/actions/runs/123"
            },
            {
              "test_name": "auth_bypass_test",
              "status": "pass",
              "last_run": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "failure_rate": 0.0,
              "severity": "critical",
              "details": "Authentication bypass prevention working correctly",
              "run_url": "https://github.com/org/repo/actions/runs/124"
            },
            {
              "test_name": "pii_leak_test",
              "status": "pass",
              "last_run": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "failure_rate": 0.0,
              "severity": "critical",
              "details": "PII leak prevention working correctly",
              "run_url": "https://github.com/org/repo/actions/runs/125"
            }
          ]
          EOF

          # Create sample performance data
          mkdir -p performance-data
          cat > performance-data/latest.json << EOF
          {
            "metrics": {
              "latency_p95": 1.2,
              "latency_p99": 3.1,
              "throughput": 150.0,
              "error_rate": 0.015,
              "availability": 0.9995,
              "cost_per_1k": 0.038
            },
            "trends": {
              "latency_trend": "improving",
              "throughput_trend": "stable",
              "cost_trend": "decreasing"
            },
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

      - name: Run report generation
        run: |
          # Create reports directory with timestamp
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          WEEK_NUMBER=$(date -u +%V)
          YEAR=$(date -u +%Y)
          REPORT_DIR="${{ env.REPORT_OUTPUT_DIR }}/${YEAR}-W${WEEK_NUMBER}"
          
          mkdir -p "$REPORT_DIR"
          
          # Run the enhanced report generator
          python testbed/tools/reporter/generate_testbed_report.py \
            --config config/report-config.yaml \
            --output "$REPORT_DIR" \
            --format ${{ github.event.inputs.report_format || 'both' }} \
            --time-range ${{ github.event.inputs.time_range || '168' }} \
            --timestamp "$TIMESTAMP" \
            --include-charts \
            --include-trends \
            --include-recommendations
          
          # Create report index
          cat > "$REPORT_DIR/index.html" << EOF
          <!DOCTYPE html>
          <html>
          <head>
              <title>Testbed Report ${YEAR}-W${WEEK_NUMBER}</title>
              <meta charset="utf-8">
              <meta name="viewport" content="width=device-width, initial-scale=1">
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
                  .header { background-color: #2196F3; color: white; padding: 20px; border-radius: 5px; margin-bottom: 30px; }
                  .report-link { display: block; margin: 15px 0; padding: 15px; background-color: #f5f5f5; border-radius: 5px; text-decoration: none; color: #333; }
                  .report-link:hover { background-color: #e0e0e0; }
                  .timestamp { color: #666; font-size: 0.9em; }
              </style>
          </head>
          <body>
              <div class="header">
                  <h1>Provability Fabric Testbed Report</h1>
                  <p><strong>Week:</strong> ${YEAR}-W${WEEK_NUMBER}</p>
                  <p><strong>Generated:</strong> <span class="timestamp">${TIMESTAMP}</span></p>
              </div>
              
              <h2>Available Reports</h2>
              
              <a href="testbed_report.html" class="report-link">
                  <h3>üìä HTML Report</h3>
                  <p>Interactive HTML version with charts and navigation</p>
              </a>
              
              <a href="testbed_report.pdf" class="report-link">
                  <h3>üìÑ PDF Report</h3>
                  <p>Printable PDF version for offline review</p>
              </a>
              
              <a href="report_summary.json" class="report-link">
                  <h3>üîß JSON Summary</h3>
                  <p>Machine-readable summary for automation</p>
              </a>
              
              <a href="report_summary.md" class="report-link">
                  <h3>üìù Markdown Summary</h3>
                  <p>Markdown version for documentation</p>
              </a>
              
              <div style="margin-top: 40px; padding: 20px; background-color: #f9f9f9; border-radius: 5px;">
                  <h3>üìà Key Metrics</h3>
                  <p>This report covers the week of ${YEAR}-W${WEEK_NUMBER} and includes comprehensive analysis of:</p>
                  <ul>
                      <li>Performance metrics (P95/P99 latencies, throughput)</li>
                      <li>Security metrics (block rates, vulnerabilities, red-team results)</li>
                      <li>Cost analysis (cost per 1k transactions)</li>
                      <li>Reliability metrics (availability, error rates)</li>
                      <li>ART harness comparison</li>
                      <li>Security posture assessment</li>
                  </ul>
              </div>
          </body>
          </html>
          EOF
          
          echo "Reports generated in $REPORT_DIR"

      - name: Validate report output
        run: |
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          WEEK_NUMBER=$(date -u +%V)
          YEAR=$(date -u +%Y)
          REPORT_DIR="${{ env.REPORT_OUTPUT_DIR }}/${YEAR}-W${WEEK_NUMBER}"
          
          # Check that all required files were generated
          required_files = [
              "testbed_report.html",
              "testbed_report.pdf", 
              "report_summary.json",
              "report_summary.md",
              "index.html"
          ]
          
          missing_files = []
          for file in required_files:
              if not os.path.exists(f"{REPORT_DIR}/{file}"):
                  missing_files.append(file)
          
          if missing_files:
              print(f"ERROR: Missing required files: {missing_files}")
              exit(1)
          
          # Validate report summary contains all required KPIs
          import json
          
          with open(f'{REPORT_DIR}/report_summary.json', 'r') as f:
              summary = json.load(f)
          
          required_kpis = [
              'latency_p95', 'latency_p99', 'block_rate',
              'cost_per_1k_transactions', 'confidence_score',
              'availability', 'throughput', 'error_rate'
          ]
          
          metrics = summary.get('metrics_summary', {})
          missing_kpis = []
          
          for kpi in required_kpis:
              if kpi not in metrics.get('performance', {}) and \
                 kpi not in metrics.get('security', {}) and \
                 kpi not in metrics.get('cost', {}) and \
                 kpi not in metrics.get('reliability', {}):
                  missing_kpis.append(kpi)
          
          if missing_kpis:
              print(f'ERROR: Missing required KPIs: {missing_kpis}')
              exit(1)
          
          print('All required KPIs present in report')
          print(f'‚úÖ Report validation passed for {REPORT_DIR}')

      - name: Upload reports as artifacts
        uses: actions/upload-artifact@v3
        with:
          name: testbed-reports-${{ github.run_number }}
          path: ${{ env.REPORT_OUTPUT_DIR }}/
          retention-days: 90

      - name: Create report summary
        run: |
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          WEEK_NUMBER=$(date -u +%V)
          YEAR=$(date -u +%Y)
          REPORT_DIR="${{ env.REPORT_OUTPUT_DIR }}/${YEAR}-W${WEEK_NUMBER}"
          
          # Create a summary for the GitHub run
          cat > report-summary.md << EOF
          # Testbed Report Summary

          **Generated:** ${TIMESTAMP}
          **Week:** ${YEAR}-W${WEEK_NUMBER}
          **Run ID:** ${{ github.run_number }}
          **Commit:** ${{ github.sha }}

          ## Generated Files
          - HTML Report: \`testbed_report.html\`
          - PDF Report: \`testbed_report.pdf\`
          - Summary: \`report_summary.json\`
          - Markdown: \`report_summary.md\`
          - Index: \`index.html\`

          ## Report Location
          Reports are available in: \`${REPORT_DIR}\`

          ## Key Metrics
          EOF

          # Extract key metrics from summary
          python -c "
          import json
          
          with open('${REPORT_DIR}/report_summary.json', 'r') as f:
              summary = json.load(f)
          
          metrics = summary.get('metrics_summary', {})
          
          print(f'**P95 Latency:** {metrics.get(\"performance\", {}).get(\"latency_p95\", \"N/A\")}s')
          print(f'**P99 Latency:** {metrics.get(\"performance\", {}).get(\"latency_p99\", \"N/A\")}s')
          print(f'**Block Rate:** {metrics.get(\"security\", {}).get(\"block_rate\", \"N/A\")}%')
          print(f'**Cost per 1K:** ${metrics.get(\"cost\", {}).get(\"cost_per_1k_transactions\", \"N/A\")}')
          print(f'**Confidence:** {metrics.get(\"confidence\", {}).get(\"confidence_score\", \"N/A\")}%')
          print(f'**Availability:** {metrics.get(\"reliability\", {}).get(\"availability\", \"N/A\")}%')
          " >> report-summary.md

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('report-summary.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Update status check
        run: |
          # Update status check to indicate report generation success
          echo "Report generation completed successfully"
          echo "All required KPIs present"
          echo "Reports generated in ${{ env.REPORT_OUTPUT_DIR }}"

    outputs:
      report_files: ${{ steps.generate-report.outputs.report_files }}
      report_summary: ${{ steps.create-summary.outputs.summary }}

  validate-report:
    name: Validate Report Quality
    runs-on: ubuntu-latest
    needs: generate-report

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download reports
        uses: actions/download-artifact@v3
        with:
          name: testbed-reports-${{ github.run_number }}
          path: ./reports

      - name: Validate HTML report
        run: |
          # Find the latest report directory
          LATEST_REPORT=$(ls -td reports/*/ | head -1)
          echo "Validating report in: $LATEST_REPORT"
          
          # Check HTML report structure
          if ! grep -q "Provability Fabric Testbed Report" "$LATEST_REPORT/testbed_report.html"; then
            echo "ERROR: HTML report missing title"
            exit 1
          fi

          if ! grep -q "Performance Metrics" "$LATEST_REPORT/testbed_report.html"; then
            echo "ERROR: HTML report missing performance section"
            exit 1
          fi

          if ! grep -q "Security Metrics" "$LATEST_REPORT/testbed_report.html"; then
            echo "ERROR: HTML report missing security section"
            exit 1
          fi

          echo "HTML report validation passed"

      - name: Validate PDF report
        run: |
          LATEST_REPORT=$(ls -td reports/*/ | head -1)
          
          # Check PDF report exists and has content
          if [ ! -s "$LATEST_REPORT/testbed_report.pdf" ]; then
            echo "ERROR: PDF report is empty or missing"
            exit 1
          fi

          echo "PDF report validation passed"

      - name: Validate JSON summary
        run: |
          LATEST_REPORT=$(ls -td reports/*/ | head -1)
          
          # Validate JSON structure and required fields
          python -c "
          import json
          import sys

          with open('$LATEST_REPORT/report_summary.json', 'r') as f:
              summary = json.load(f)

          required_fields = [
              'report_date', 'time_range_hours', 'metrics_summary',
              'art_comparison_count', 'redteam_tests_count'
          ]

          for field in required_fields:
              if field not in summary:
                  print(f'ERROR: Missing required field: {field}')
                  sys.exit(1)

          print('JSON summary validation passed')
          "

      - name: Performance validation
        run: |
          LATEST_REPORT=$(ls -td reports/*/ | head -1)
          
          # Validate performance metrics are within acceptable ranges
          python -c "
          import json

          with open('$LATEST_REPORT/report_summary.json', 'r') as f:
              summary = json.load(f)

          metrics = summary.get('metrics_summary', {})
          performance = metrics.get('performance', {})

          # Check latency thresholds
          p95 = performance.get('latency_p95', 0)
          p99 = performance.get('latency_p99', 0)

          if p95 > 10:
              print(f'WARNING: P95 latency {p95}s exceeds 10s threshold')

          if p99 > 20:
              print(f'WARNING: P99 latency {p99}s exceeds 20s threshold')

          print('Performance validation completed')
          "

      - name: Security validation
        run: |
          LATEST_REPORT=$(ls -td reports/*/ | head -1)
          
          # Validate security metrics
          python -c "
          import json

          with open('$LATEST_REPORT/report_summary.json', 'r') as f:
              summary = json.load(f)

          metrics = summary.get('metrics_summary', {})
          security = metrics.get('security', {})

          # Check security thresholds
          cross_tenant = security.get('cross_tenant_interactions', 0)
          data_leaks = security.get('data_leaks', 0)

          if cross_tenant > 0:
              print(f'CRITICAL: Cross-tenant interactions detected: {cross_tenant}')
              exit(1)

          if data_leaks > 0:
              print(f'CRITICAL: Data leaks detected: {data_leaks}')
              exit(1)

          print('Security validation passed')
          "

  publish-report:
    name: Publish Report to GitHub Pages
    runs-on: ubuntu-latest
    needs: [generate-report, validate-report]
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download reports
        uses: actions/download-artifact@v3
        with:
          name: testbed-reports-${{ github.run_number }}
          path: ./reports

      - name: Setup Pages
        uses: actions/configure-pages@v3

      - name: Upload to Pages
        uses: actions/upload-pages-artifact@v2
        with:
          path: ./reports

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2

      - name: Create release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: report-${{ github.run_number }}
          release_name: Testbed Report ${{ github.run_number }}
          body: |
            ## Testbed Report ${{ github.run_number }}

            Generated on $(date -u +%Y-%m-%dT%H:%M:%SZ)

            ### Files
            - HTML Report: `testbed_report.html`
            - PDF Report: `testbed_report.pdf`
            - Summary: `report_summary.json`
            - Markdown: `report_summary.md`

            ### Key Metrics
            See the attached files for detailed metrics and analysis.

            This report provides comprehensive insights into the Provability Fabric Testbed performance, security, and cost metrics.

            ### GitHub Pages
            Reports are also available at: ${{ steps.deployment.outputs.page_url }}
          draft: false
          prerelease: false

      - name: Upload release assets
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create-release.outputs.upload_url }}
          asset_path: ./reports/*/testbed_report.html
          asset_name: testbed_report.html
          asset_content_type: text/html

      - name: Upload PDF asset
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create-release.outputs.upload_url }}
          asset_path: ./reports/*/testbed_report.pdf
          asset_name: testbed_report.pdf
          asset_content_type: application/pdf

      - name: Upload summary asset
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create-release.outputs.upload_url }}
          asset_path: ./reports/*/report_summary.json
          asset_name: report_summary.json
          asset_content_type: application/json

      - name: Notify stakeholders
        run: |
          echo "Report published successfully"
          echo "Release URL: ${{ steps.create-release.outputs.html_url }}"
          echo "GitHub Pages URL: ${{ steps.deployment.outputs.page_url }}"
          echo "Report files uploaded as release assets"

  report-summary:
    name: Report Summary
    runs-on: ubuntu-latest
    needs: [generate-report, validate-report, publish-report]
    if: always()

    steps:
      - name: Generate report summary
        run: |
          echo "## Weekly Report Generation Summary" >> report-summary.md
          echo "" >> report-summary.md
          echo "**Report Generation:** ${{ needs.generate-report.result }}" >> report-summary.md
          echo "**Report Validation:** ${{ needs.validate-report.result }}" >> report-summary.md
          echo "**Report Publication:** ${{ needs.publish-report.result }}" >> report-summary.md
          echo "" >> report-summary.md
          
          if [ "${{ needs.publish-report.result }}" == "success" ]; then
            echo "‚úÖ **Overall Report Status: PUBLISHED**" >> report-summary.md
            echo "" >> report-summary.md
            echo "Weekly testbed report successfully generated and published:" >> report-summary.md
            echo "- HTML and PDF reports created" >> report-summary.md
            echo "- Reports validated for quality and completeness" >> report-summary.md
            echo "- Reports published to GitHub Pages" >> report-summary.md
            echo "- Release created with report assets" >> report-summary.md
          else
            echo "‚ùå **Overall Report Status: FAILED**" >> report-summary.md
            echo "" >> report-summary.md
            echo "Report generation or publication encountered failures. Check the logs for details." >> report-summary.md
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('report-summary.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });


