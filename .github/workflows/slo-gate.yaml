name: SLO Gate - k6 Load Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      load_duration:
        description: 'Load test duration in minutes'
        required: false
        default: '10'
        type: string
      virtual_users:
        description: 'Number of virtual users'
        required: false
        default: '50'
        type: string

env:
  NODE_VERSION: '18'
  K6_VERSION: '0.47.0'
  LOAD_DURATION: ${{ github.event.inputs.load_duration || '10' }}
  VIRTUAL_USERS: ${{ github.event.inputs.virtual_users || '50' }}

jobs:
  slo-load-test:
    name: SLO Load Testing
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: pf_testbed_slo
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        submodules: recursive
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: Setup Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver-opts: |
          image=moby/buildkit:v0.12.0
    
    - name: Install k6
      run: |
        # Install k6
        curl -L https://github.com/grafana/k6/releases/download/v${{ env.K6_VERSION }}/k6-v${{ env.K6_VERSION }}-linux-amd64.tar.gz | tar xz
        sudo cp k6-v${{ env.K6_VERSION }}-linux-amd64/k6 /usr/local/bin/
        k6 version
        
        # Install k6 extensions
        go install go.k6.io/xk6/cmd/xk6@latest
    
    - name: Build and start services
      run: |
        # Build services
        docker-compose build --no-cache --parallel
        
        # Start services
        docker-compose up -d
        
        # Wait for services to be healthy
        echo "Waiting for services to be ready..."
        
        # Wait for Gateway
        timeout 120 bash -c 'until curl -f http://localhost:3003/health; do sleep 2; done'
        
        # Wait for Ingress
        timeout 120 bash -c 'until curl -f http://localhost:3001/health; do sleep 2; done'
        
        # Wait for Ledger
        timeout 120 bash -c 'until curl -f http://localhost:3002/health; do sleep 2; done'
        
        # Wait for Prometheus
        timeout 120 bash -c 'until curl -f http://localhost:9090/-/healthy; do sleep 2; done'
        
        echo "All services are healthy"
    
    - name: Verify service readiness
      run: |
        # Check all service endpoints
        services=(
          "http://localhost:3003/health"
          "http://localhost:3001/health"
          "http://localhost:3002/health"
          "http://localhost:9090/-/healthy"
        )
        
        for service in "${services[@]}"; do
          echo "Checking $service..."
          response=$(curl -s -o /dev/null -w "%{http_code}" "$service")
          if [ "$response" != "200" ]; then
            echo "ERROR: $service returned $response"
            exit 1
          fi
          echo "✅ $service is ready"
        done
    
    - name: Run baseline performance test
      run: |
        # Run a quick baseline test to ensure services are responsive
        k6 run testbed/tests/performance/baseline-test.js \
          --out json=baseline-test.json \
          --duration 1m \
          --vus 5
        
        # Validate baseline performance
        python -c "
        import json
        
        with open('baseline-test.json', 'r') as f:
            metrics = json.load(f)
        
        # Check if baseline test passed
        p95 = metrics.get('metrics', {}).get('http_req_duration', {}).get('p(95)', 0)
        error_rate = metrics.get('metrics', {}).get('http_req_failed', {}).get('rate', 0)
        
        if p95 > 2000:  # 2 seconds
            print(f'ERROR: Baseline P95 latency {p95}ms exceeds 2s threshold')
            exit(1)
        
        if error_rate > 0.01:  # 1%
            print(f'ERROR: Baseline error rate {error_rate} exceeds 1% threshold')
            exit(1)
        
        print('✅ Baseline performance test passed')
        "
    
    - name: Run SLO load test
      run: |
        # Set SLO thresholds
        export K6_SLO_P95_THRESHOLD=5000  # 5 seconds
        export K6_SLO_P99_THRESHOLD=10000  # 10 seconds
        export K6_SLO_ERROR_RATE_THRESHOLD=0.02  # 2%
        
        # Run the main SLO load test
        k6 run testbed/tests/performance/slo-load-test.js \
          --out json=slo-load-test.json \
          --out influxdb=http://localhost:8086/k6 \
          --duration ${LOAD_DURATION}m \
          --vus ${VIRTUAL_USERS} \
          --stage 30s:${VIRTUAL_USERS} \
          --stage ${LOAD_DURATION}m:${VIRTUAL_USERS} \
          --stage 30s:0
        
        echo "SLO load test completed"
    
    - name: Validate SLO thresholds
      run: |
        # Parse k6 results and validate against SLOs
        python -c "
        import json
        import sys
        
        # Load test results
        with open('slo-load-test.json', 'r') as f:
            metrics = json.load(f)
        
        # Extract key metrics
        http_metrics = metrics.get('metrics', {})
        
        # Latency metrics
        p95 = http_metrics.get('http_req_duration', {}).get('p(95)', 0)
        p99 = http_metrics.get('http_req_duration', {}).get('p(99)', 0)
        
        # Error metrics
        error_rate = http_metrics.get('http_req_failed', {}).get('rate', 0)
        
        # Throughput metrics
        requests_per_sec = http_metrics.get('http_reqs', {}).get('rate', 0)
        
        print(f'P95 Latency: {p95:.2f}ms')
        print(f'P99 Latency: {p99:.2f}ms')
        print(f'Error Rate: {error_rate:.4f}')
        print(f'Throughput: {requests_per_sec:.2f} req/s')
        
        # SLO validation
        slo_violations = []
        
        if p95 > 5000:  # 5 seconds
            slo_violations.append(f'P95 latency {p95:.2f}ms exceeds 5s threshold')
        
        if p99 > 10000:  # 10 seconds
            slo_violations.append(f'P99 latency {p99:.2f}ms exceeds 10s threshold')
        
        if error_rate > 0.02:  # 2%
            slo_violations.append(f'Error rate {error_rate:.4f} exceeds 2% threshold')
        
        if requests_per_sec < 10:  # Minimum 10 req/s
            slo_violations.append(f'Throughput {requests_per_sec:.2f} req/s below 10 req/s minimum')
        
        if slo_violations:
            print('❌ SLO violations detected:')
            for violation in slo_violations:
                print(f'  - {violation}')
            sys.exit(1)
        
        print('✅ All SLO thresholds met')
        "
    
    - name: Generate performance report
      run: |
        # Create performance report
        mkdir -p slo-artifacts
        
        # Generate HTML report
        python -c "
        import json
        import html
        
        with open('slo-load-test.json', 'r') as f:
            metrics = json.load(f)
        
        http_metrics = metrics.get('metrics', {})
        
        html_content = f'''
        <!DOCTYPE html>
        <html>
        <head>
            <title>SLO Load Test Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                .metric {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .pass {{ border-color: #4CAF50; background-color: #f1f8e9; }}
                .fail {{ border-color: #f44336; background-color: #ffebee; }}
                .header {{ background-color: #2196F3; color: white; padding: 20px; border-radius: 5px; }}
            </style>
        </head>
        <body>
            <div class='header'>
                <h1>SLO Load Test Report</h1>
                <p>Generated: {metrics.get('state', {}).get('testRunDuration', 'N/A')}</p>
            </div>
            
            <div class='metric'>
                <h3>Latency Metrics</h3>
                <p><strong>P95:</strong> {http_metrics.get('http_req_duration', {}).get('p(95)', 0):.2f}ms</p>
                <p><strong>P99:</strong> {http_metrics.get('http_req_duration', {}).get('p(99)', 0):.2f}ms</p>
            </div>
            
            <div class='metric'>
                <h3>Error Metrics</h3>
                <p><strong>Error Rate:</strong> {http_metrics.get('http_req_failed', {}).get('rate', 0):.4f}</p>
            </div>
            
            <div class='metric'>
                <h3>Throughput Metrics</h3>
                <p><strong>Requests/sec:</strong> {http_metrics.get('http_reqs', {}).get('rate', 0):.2f}</p>
            </div>
        </body>
        </html>
        '''
        
        with open('slo-artifacts/performance-report.html', 'w') as f:
            f.write(html_content)
        
        print('Performance report generated')
        "
        
        # Copy test artifacts
        cp slo-load-test.json slo-artifacts/
        cp baseline-test.json slo-artifacts/
        
        # Generate summary
        cat > slo-artifacts/slo-summary.md << EOF
        # SLO Load Test Summary
        
        **Test Date:** $(date -u +%Y-%m-%dT%H:%M:%SZ)
        **Duration:** ${LOAD_DURATION} minutes
        **Virtual Users:** ${VIRTUAL_USERS}
        **Git Commit:** ${{ github.sha }}
        
        ## Test Results
        - **Baseline Test:** ✅ Passed
        - **SLO Load Test:** ✅ Completed
        - **SLO Validation:** ✅ All thresholds met
        
        ## SLO Thresholds
        - **P95 Latency:** ≤ 5 seconds
        - **P99 Latency:** ≤ 10 seconds
        - **Error Rate:** ≤ 2%
        - **Minimum Throughput:** ≥ 10 req/s
        
        ## Performance Metrics
        See performance-report.html for detailed metrics.
        EOF
    
    - name: Upload SLO artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: slo-artifacts-${{ github.run_number }}
        path: slo-artifacts/
        retention-days: 30
    
    - name: SLO gate validation
      run: |
        # Final SLO gate validation
        echo "Validating SLO gate..."
        
        # Check if load test completed successfully
        if [ ! -f "slo-load-test.json" ]; then
          echo "ERROR: SLO load test results not found"
          exit 1
        fi
        
        # Validate SLO thresholds one more time
        python -c "
        import json
        
        with open('slo-load-test.json', 'r') as f:
            metrics = json.load(f)
        
        http_metrics = metrics.get('metrics', {})
        p95 = http_metrics.get('http_req_duration', {}).get('p(95)', 0)
        p99 = http_metrics.get('http_req_duration', {}).get('p(99)', 0)
        error_rate = http_metrics.get('http_req_failed', {}).get('rate', 0)
        
        if p95 > 5000 or p99 > 10000 or error_rate > 0.02:
            print('ERROR: SLO thresholds exceeded')
            exit(1)
        
        print('✅ SLO gate validation passed')
        "
        
        echo "✅ SLO gate passed - all performance thresholds met"
    
    - name: Cleanup services
      if: always()
      run: |
        docker-compose down -v --remove-orphans
        docker system prune -f
    
    outputs:
      slo-status: ${{ job.status }}
      performance-metrics: ${{ steps.slo-validation.outputs.metrics }}

  slo-summary:
    name: SLO Summary
    runs-on: ubuntu-latest
    needs: slo-load-test
    if: always()
    
    steps:
    - name: Generate SLO summary
      run: |
        echo "## SLO Gate Summary" >> slo-summary.md
        echo "" >> slo-summary.md
        echo "**SLO Load Test:** ${{ needs.slo-load-test.result }}" >> slo-summary.md
        echo "**Test Duration:** ${LOAD_DURATION} minutes" >> slo-summary.md
        echo "**Virtual Users:** ${VIRTUAL_USERS}" >> slo-summary.md
        echo "" >> slo-summary.md
        
        if [ "${{ needs.slo-load-test.result }}" == "success" ]; then
          echo "✅ **SLO Status: PASSED**" >> slo-summary.md
          echo "" >> slo-summary.md
          echo "All SLO thresholds met:" >> slo-summary.md
          echo "- P95 latency ≤ 5 seconds" >> slo-summary.md
          echo "- P99 latency ≤ 10 seconds" >> slo-summary.md
          echo "- Error rate ≤ 2%" >> slo-summary.md
          echo "- Throughput ≥ 10 req/s" >> slo-summary.md
        else
          echo "❌ **SLO Status: FAILED**" >> slo-summary.md
          echo "" >> slo-summary.md
          echo "SLO thresholds exceeded. Check the performance report for details." >> slo-summary.md
        fi
    
    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('slo-summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });
