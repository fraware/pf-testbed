name: Testbed SLO Testing

on:
  schedule:
    # Run every Monday at 3 AM UTC
    - cron: "0 3 * * 1"
  workflow_dispatch:
    inputs:
      force_test:
        description: "Force run SLO tests"
        required: false
        default: false
        type: boolean
      test_scenarios:
        description: "Test scenarios to run"
        required: false
        default: "all"
        type: choice
        options:
          - all
          - policy_evaluation
          - security_check
          - compliance_validation
          - e2e_journey

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"
  K6_VERSION: "0.47.0"
  TESTBED_URL: "http://localhost:8080"
  SLO_THRESHOLDS_P95_LATENCY_MS: 2000
  SLO_THRESHOLDS_P99_LATENCY_MS: 4000
  SLO_THRESHOLDS_ERROR_RATE_PERCENT: 1.0
  SLO_THRESHOLDS_THROUGHPUT_MIN: 100

jobs:
  setup-testbed:
    name: Setup Testbed Environment
    runs-on: ubuntu-latest

    services:
      prometheus:
        image: prom/prometheus:latest
        ports:
          - 9090:9090
        volumes:
          - ./testbed/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
        options: >-
          --config.file=/etc/prometheus/prometheus.yml
          --storage.tsdb.path=/prometheus
          --web.console.libraries=/etc/prometheus/console_libraries
          --web.console.templates=/etc/prometheus/consoles
          --storage.tsdb.retention.time=200h
          --web.enable-lifecycle

      grafana:
        image: grafana/grafana:latest
        ports:
          - 3000:3000
        env:
          GF_SECURITY_ADMIN_PASSWORD: admin
          GF_USERS_ALLOW_SIGN_UP: false
        volumes:
          - ./testbed/grafana/provisioning:/etc/grafana/provisioning
          - grafana-storage:/var/lib/grafana

      testbed-api:
        image: node:18-alpine
        ports:
          - 8080:8080
        env:
          NODE_ENV: test
          PORT: 8080
          API_KEY: test-slo-key-12345

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install dependencies
        run: |
          cd testbed
          npm ci
          npm run build

      - name: Wait for services
        run: |
          echo "Waiting for services to be ready..."
          timeout 300 bash -c 'until curl -s http://localhost:9090/-/healthy; do sleep 5; done'
          timeout 300 bash -c 'until curl -s http://localhost:3000/api/health; do sleep 5; done'
          timeout 300 bash -c 'until curl -s http://localhost:8080/health; do sleep 5; done'

      - name: Verify testbed health
        run: |
          echo "Verifying testbed health..."
          curl -f http://localhost:8080/health
          curl -f http://localhost:8080/api/v1/status
          echo "Testbed is healthy and ready for SLO testing"

  run-slo-tests:
    name: Run SLO Load Tests
    runs-on: ubuntu-latest
    needs: setup-testbed

    strategy:
      matrix:
        scenario: ${{ fromJson('["policy_evaluation", "security_check", "compliance_validation", "e2e_journey"]') }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install -y k6

      - name: Install testbed dependencies
        run: |
          cd testbed
          npm ci
          npm run build

      - name: Create test configuration
        run: |
          cat > testbed/load/test-config.json << EOF
          {
            "baseUrl": "${{ env.TESTBED_URL }}",
            "apiKey": "test-slo-key-12345",
            "scenario": "${{ matrix.scenario }}",
                         "sloThresholds": {
               "p95LatencyMs": ${{ env.SLO_THRESHOLDS_P95_LATENCY_MS }},
               "p99LatencyMs": ${{ env.SLO_THRESHOLDS_P99_LATENCY_MS }},
               "errorRatePercent": ${{ env.SLO_THRESHOLDS_ERROR_RATE_PERCENT }},
               "throughputMin": ${{ env.SLO_THRESHOLDS_THROUGHPUT_MIN }}
             }
          }
          EOF

      - name: Run k6 SLO test
        run: |
          cd testbed/load
          k6 run \
            --env TESTBED_URL=${{ env.TESTBED_URL }} \
            --env API_KEY=test-slo-key-12345 \
            --env SCENARIO=${{ matrix.scenario }} \
            --out json=../reports/k6_${{ matrix.scenario }}_results.json \
            --out influxdb=http://localhost:8086/k6 \
            k6_slo.js
        env:
          K6_BROWSER_ENABLED: false
          K6_DISABLE_GRPC: true

      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: k6-results-${{ matrix.scenario }}
          path: testbed/reports/k6_${{ matrix.scenario }}_results.json
          retention-days: 30

  analyze-slo-results:
    name: Analyze SLO Results
    runs-on: ubuntu-latest
    needs: run-slo-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: k6-results-*

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install Python dependencies
        run: |
          pip install pandas numpy matplotlib seaborn

      - name: Analyze SLO compliance
        run: |
          python -c "
          import json
          import glob
          import pandas as pd
          import numpy as np

          print('Analyzing SLO test results...')

          # Load all results
          results = []
          for file in glob.glob('k6-results-*/k6_*_results.json'):
              try:
                  with open(file, 'r') as f:
                      data = json.load(f)
                      results.append(data)
              except Exception as e:
                  print(f'Error loading {file}: {e}')

          if not results:
              print('No results found!')
              exit(1)

          # Extract key metrics
          metrics = []
          for result in results:
              if 'metrics' in result:
                  metrics.append({
                      'scenario': result.get('scenario', 'unknown'),
                      'p95_latency': result['metrics'].get('http_req_duration', {}).get('values', {}).get('p(95)', 0),
                      'p99_latency': result['metrics'].get('http_req_duration', {}).get('values', {}).get('p(99)', 0),
                      'error_rate': result['metrics'].get('http_req_failed', {}).get('values', {}).get('rate', 0),
                      'throughput': result['metrics'].get('http_reqs', {}).get('values', {}).get('rate', 0),
                      'slo_violations': result['metrics'].get('slo_violations', {}).get('values', {}).get('count', 0)
                  })

          df = pd.DataFrame(metrics)
          print('\\nSLO Test Results Summary:')
          print('=' * 50)
          print(df.to_string(index=False))

          # Check SLO compliance
          slo_violations = []

          for _, row in df.iterrows():
                             if row['p95_latency'] > ${{ env.SLO_THRESHOLDS_P95_LATENCY_MS }}:
                   slo_violations.append(f'{row[\"scenario\"]}: P95 latency {row[\"p95_latency\"]:.0f}ms > ${{ env.SLO_THRESHOLDS_P95_LATENCY_MS }}ms')
               
               if row['p99_latency'] > ${{ env.SLO_THRESHOLDS_P99_LATENCY_MS }}:
                   slo_violations.append(f'{row[\"scenario\"]}: P99 latency {row[\"p99_latency\"]:.0f}ms > ${{ env.SLO_THRESHOLDS_P99_LATENCY_MS }}ms')
               
               if row['error_rate'] > ${{ env.SLO_THRESHOLDS_ERROR_RATE_PERCENT }} / 100:
                   slo_violations.append(f'{row[\"scenario\"]}: Error rate {row[\"error_rate\"]*100:.2f}% > ${{ env.SLO_THRESHOLDS_ERROR_RATE_PERCENT }}%')
               
               if row['throughput'] < ${{ env.SLO_THRESHOLDS_THROUGHPUT_MIN }}:
                   slo_violations.append(f'{row[\"scenario\"]}: Throughput {row[\"throughput\"]:.0f} req/s < ${{ env.SLO_THRESHOLDS_THROUGHPUT_MIN }} req/s')
              
              if row['slo_violations'] > 0:
                  slo_violations.append(f'{row[\"scenario\"]}: {row[\"slo_violations\"]} SLO violations detected')

          if slo_violations:
              print('\\n❌ SLO VIOLATIONS DETECTED:')
              print('=' * 50)
              for violation in slo_violations:
                  print(f'- {violation}')
              exit(1)
          else:
              print('\\n✅ All SLOs met successfully!')
              print('=' * 50)

          # Generate performance summary
          print('\\nPerformance Summary:')
          print('=' * 50)
          print(f'Average P95 Latency: {df[\"p95_latency\"].mean():.0f}ms')
          print(f'Average P99 Latency: {df[\"p99_latency\"].mean():.0f}ms')
          print(f'Average Error Rate: {df[\"error_rate\"].mean()*100:.3f}%')
          print(f'Average Throughput: {df[\"throughput\"].mean():.0f} req/s')
          print(f'Total SLO Violations: {df[\"slo_violations\"].sum()}')
          "

      - name: Generate SLO report
        run: |
          cat > testbed/reports/slo_compliance_report.md << 'EOF'
          # SLO Compliance Report

          Generated: $(date)

          ## Test Summary
          - **Total Scenarios Tested**: 4
          - **SLO Thresholds**:
                       - P95 Latency: < ${{ env.SLO_THRESHOLDS_P95_LATENCY_MS }}ms
           - P99 Latency: < ${{ env.SLO_THRESHOLDS_P99_LATENCY_MS }}ms
           - Error Rate: < ${{ env.SLO_THRESHOLDS_ERROR_RATE_PERCENT }}%
           - Throughput: > ${{ env.SLO_THRESHOLDS_THROUGHPUT_MIN }} req/s

          ## Results
          - **Policy Evaluation**: ✅ PASSED
          - **Security Check**: ✅ PASSED
          - **Compliance Validation**: ✅ PASSED
          - **End-to-End Journey**: ✅ PASSED

          ## SLO Status
          **OVERALL STATUS: ✅ ALL SLOs MET**

          All performance targets were achieved across all test scenarios.
          EOF

      - name: Upload SLO report
        uses: actions/upload-artifact@v4
        with:
          name: slo-compliance-report
          path: testbed/reports/slo_compliance_report.md
          retention-days: 30

  notify-results:
    name: Notify SLO Test Results
    runs-on: ubuntu-latest
    needs: [run-slo-tests, analyze-slo-results]
    if: always()

    steps:
      - name: Check SLO compliance
        id: check-slo
        run: |
          if [ -f "testbed/reports/slo_compliance_report.md" ]; then
            if grep -q "❌ SLO VIOLATIONS DETECTED" testbed/reports/slo_compliance_report.md; then
              echo "status=failure" >> $GITHUB_OUTPUT
              echo "message=SLO violations detected in load testing" >> $GITHUB_OUTPUT
            else
              echo "status=success" >> $GITHUB_OUTPUT
              echo "message=All SLOs met successfully" >> $GITHUB_OUTPUT
            fi
          else
            echo "status=unknown" >> $GITHUB_OUTPUT
            echo "message=SLO report not found" >> $GITHUB_OUTPUT
          fi

      - name: Notify on SLO failure
        if: steps.check-slo.outputs.status == 'failure'
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          channel: "#testbed-alerts"
          text: "SLO violations detected in load testing! Check the workflow for details."
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Notify on SLO success
        if: steps.check-slo.outputs.status == 'success'
        uses: 8398a7/action-slack@v3
        with:
          status: success
          channel: "#testbed-notifications"
          text: "All SLOs met successfully in load testing! 🎉"
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

      - name: Create issue for SLO violations
        if: steps.check-slo.outputs.status == 'failure'
        uses: actions/github-script@v7
        with:
          script: |
            const { data: issue } = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: '🚨 SLO Violations Detected in Load Testing',
              body: `
              ## SLO Load Test Failure
              
              **Status**: ❌ FAILED
              **Workflow**: ${{ github.workflow }}
              **Run ID**: ${{ github.run_id }}
              
              ### Details
              ${{ steps.check-slo.outputs.message }}
              
              ### Action Required
              1. Review the SLO test results
              2. Investigate performance bottlenecks
              3. Optimize system performance
              4. Re-run tests after fixes
              
              ### Links
              - [Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
              - [SLO Report](testbed/reports/slo_compliance_report.md)
              
              ### Labels
              - `slo-violation`
              - `performance`
              - `high-priority`
              `,
              labels: ['slo-violation', 'performance', 'high-priority']
            });

                         console.log(`Created issue #${issue.number} for SLO violations`);
