name: Testbed Report Generation

on:
  schedule:
    # Run weekly on Sundays at 2 AM UTC
    - cron: "0 2 * * 0"
  workflow_dispatch:
    # Allow manual triggering
  push:
    branches: [main]
    paths:
      - "testbed/tools/reporter/**"
      - "testbed/runtime/**"
      - "testbed/metrics/**"

env:
  PYTHON_VERSION: "3.11"
  REPORT_OUTPUT_DIR: "./reports"
  ART_RESULTS_PATH: "./art-results/latest.json"

jobs:
  generate-report:
    name: Generate Testbed Report
    runs-on: ubuntu-latest

    services:
      prometheus:
        image: prom/prometheus:latest
        ports:
          - 9090:9090
        volumes:
          - ./testbed/ops/prometheus:/etc/prometheus
          - prometheus_data:/prometheus
        command:
          - "--config.file=/etc/prometheus/prometheus.yml"
          - "--storage.tsdb.path=/prometheus"
          - "--web.console.libraries=/etc/prometheus/console_libraries"
          - "--web.console.templates=/etc/prometheus/consoles"
          - "--storage.tsdb.retention.time=200h"
          - "--web.enable-lifecycle"

      ledger:
        image: node:18-alpine
        ports:
          - 8080:8080
        working-dir: /app
        volumes:
          - ./testbed/runtime/ledger:/app
        command: |
          sh -c "
            npm install &&
            npm start
          "

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            build-essential \
            libffi-dev \
            libssl-dev \
            pkg-config

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install \
            aiohttp \
            jinja2 \
            pyyaml \
            pandas \
            matplotlib \
            seaborn \
            weasyprint

      - name: Create configuration
        run: |
          mkdir -p config
          cat > config/report-config.yaml << EOF
          prometheus_url: http://localhost:9090
          ledger_url: http://localhost:8080
          art_results_path: ${{ env.ART_RESULTS_PATH }}
          include_art_comparison: true
          include_redteam_analysis: true
          kpi_thresholds:
            latency_p95: 2.0
            latency_p99: 5.0
            block_rate: 0.01
            cross_tenant_interactions: 0
            data_leaks: 0
            cost_per_1k_transactions: 0.05
            confidence_score: 0.95
            fallback_rate: 0.05
          EOF

      - name: Wait for services
        run: |
          echo "Waiting for Prometheus to be ready..."
          timeout 60 bash -c 'until curl -s http://localhost:9090/-/ready; do sleep 2; done'

          echo "Waiting for Ledger to be ready..."
          timeout 60 bash -c 'until curl -s http://localhost:8080/health; do sleep 2; done'

      - name: Generate test data
        run: |
          # Create sample ART results for testing
          mkdir -p art-results
          cat > art-results/latest.json << EOF
          {
            "latency_p95": 1.8,
            "latency_p99": 4.5,
            "throughput": 120.0,
            "error_rate": 0.02,
            "block_rate": 0.008,
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF

          # Create sample red-team results
          mkdir -p redteam-results
          cat > redteam-results/latest.json << EOF
          [
            {
              "test_name": "injection_test",
              "status": "pass",
              "last_run": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "failure_rate": 0.0,
              "severity": "high",
              "details": "SQL injection prevention working correctly",
              "run_url": "https://github.com/org/repo/actions/runs/123"
            },
            {
              "test_name": "auth_bypass_test",
              "status": "pass",
              "last_run": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
              "failure_rate": 0.0,
              "severity": "critical",
              "details": "Authentication bypass prevention working correctly",
              "run_url": "https://github.com/org/repo/actions/runs/124"
            }
          ]
          EOF

      - name: Run report generation
        run: |
          mkdir -p ${{ env.REPORT_OUTPUT_DIR }}
          python testbed/tools/reporter/generate_testbed_report.py \
            --config config/report-config.yaml \
            --output ${{ env.REPORT_OUTPUT_DIR }} \
            --format both \
            --time-range 24

      - name: Validate report output
        run: |
          # Check that all required files were generated
          if [ ! -f "${{ env.REPORT_OUTPUT_DIR }}/testbed_report.html" ]; then
            echo "ERROR: HTML report not generated"
            exit 1
          fi

          if [ ! -f "${{ env.REPORT_OUTPUT_DIR }}/testbed_report.pdf" ]; then
            echo "ERROR: PDF report not generated"
            exit 1
          fi

          if [ ! -f "${{ env.REPORT_OUTPUT_DIR }}/report_summary.json" ]; then
            echo "ERROR: Report summary not generated"
            exit 1
          fi

          # Validate report summary contains all required KPIs
          python -c "
          import json
          import sys

          with open('${{ env.REPORT_OUTPUT_DIR }}/report_summary.json', 'r') as f:
              summary = json.load(f)

          required_kpis = [
              'latency_p95', 'latency_p99', 'block_rate',
              'cost_per_1k_transactions', 'confidence_score'
          ]

          metrics = summary.get('metrics_summary', {})
          missing_kpis = []

          for kpi in required_kpis:
              if kpi not in metrics.get('performance', {}) and \
                 kpi not in metrics.get('security', {}) and \
                 kpi not in metrics.get('cost', {}) and \
                 kpi not in metrics.get('confidence', {}):
                  missing_kpis.append(kpi)

          if missing_kpis:
              print(f'ERROR: Missing required KPIs: {missing_kpis}')
              sys.exit(1)

          print('All required KPIs present in report')
          "

      - name: Upload reports as artifacts
        uses: actions/upload-artifact@v3
        with:
          name: testbed-reports-${{ github.run_number }}
          path: ${{ env.REPORT_OUTPUT_DIR }}/
          retention-days: 30

      - name: Create report summary
        run: |
          # Create a summary for the GitHub run
          cat > report-summary.md << EOF
          # Testbed Report Summary

          **Generated:** $(date -u +%Y-%m-%dT%H:%M:%SZ)
          **Run ID:** ${{ github.run_number }}
          **Commit:** ${{ github.sha }}

          ## Generated Files
          - HTML Report: \`testbed_report.html\`
          - PDF Report: \`testbed_report.pdf\`
          - Summary: \`report_summary.json\`

          ## Key Metrics
          EOF

          # Extract key metrics from summary
          python -c "
          import json

          with open('${{ env.REPORT_OUTPUT_DIR }}/report_summary.json', 'r') as f:
              summary = json.load(f)

          metrics = summary.get('metrics_summary', {})

          print(f'**P95 Latency:** {metrics.get(\"performance\", {}).get(\"latency_p95\", \"N/A\")}s')
          print(f'**P99 Latency:** {metrics.get(\"performance\", {}).get(\"latency_p99\", \"N/A\")}s')
          print(f'**Block Rate:** {metrics.get(\"security\", {}).get(\"block_rate\", \"N/A\")}%')
          print(f'**Cost per 1K:** ${metrics.get(\"cost\", {}).get(\"cost_per_1k_transactions\", \"N/A\")}')
          print(f'**Confidence:** {metrics.get(\"confidence\", {}).get(\"confidence_score\", \"N/A\")}%')
          " >> report-summary.md

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('report-summary.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Update status check
        run: |
          # Update status check to indicate report generation success
          echo "Report generation completed successfully"
          echo "All required KPIs present"
          echo "Reports generated in ${{ env.REPORT_OUTPUT_DIR }}"

    outputs:
      report_files: ${{ steps.generate-report.outputs.report_files }}
      report_summary: ${{ steps.create-summary.outputs.summary }}

  validate-report:
    name: Validate Report Quality
    runs-on: ubuntu-latest
    needs: generate-report

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download reports
        uses: actions/download-artifact@v3
        with:
          name: testbed-reports-${{ github.run_number }}
          path: ./reports

      - name: Validate HTML report
        run: |
          # Check HTML report structure
          if ! grep -q "Provability Fabric Testbed Report" ./reports/testbed_report.html; then
            echo "ERROR: HTML report missing title"
            exit 1
          fi

          if ! grep -q "Performance Metrics" ./reports/testbed_report.html; then
            echo "ERROR: HTML report missing performance section"
            exit 1
          fi

          if ! grep -q "Security Metrics" ./reports/testbed_report.html; then
            echo "ERROR: HTML report missing security section"
            exit 1
          fi

          echo "HTML report validation passed"

      - name: Validate PDF report
        run: |
          # Check PDF report exists and has content
          if [ ! -s ./reports/testbed_report.pdf ]; then
            echo "ERROR: PDF report is empty or missing"
            exit 1
          fi

          echo "PDF report validation passed"

      - name: Validate JSON summary
        run: |
          # Validate JSON structure and required fields
          python -c "
          import json
          import sys

          with open('./reports/report_summary.json', 'r') as f:
              summary = json.load(f)

          required_fields = [
              'report_date', 'time_range_hours', 'metrics_summary',
              'art_comparison_count', 'redteam_tests_count'
          ]

          for field in required_fields:
              if field not in summary:
                  print(f'ERROR: Missing required field: {field}')
                  sys.exit(1)

          print('JSON summary validation passed')
          "

      - name: Performance validation
        run: |
          # Validate performance metrics are within acceptable ranges
          python -c "
          import json

          with open('./reports/report_summary.json', 'r') as f:
              summary = json.load(f)

          metrics = summary.get('metrics_summary', {})
          performance = metrics.get('performance', {})

          # Check latency thresholds
          p95 = performance.get('latency_p95', 0)
          p99 = performance.get('latency_p99', 0)

          if p95 > 10:
              print(f'WARNING: P95 latency {p95}s exceeds 10s threshold')

          if p99 > 20:
              print(f'WARNING: P99 latency {p99}s exceeds 20s threshold')

          print('Performance validation completed')
          "

      - name: Security validation
        run: |
          # Validate security metrics
          python -c "
          import json

          with open('./reports/report_summary.json', 'r') as f:
              summary = json.load(f)

          metrics = summary.get('metrics_summary', {})
          security = metrics.get('security', {})

          # Check security thresholds
          cross_tenant = security.get('cross_tenant_interactions', 0)
          data_leaks = security.get('data_leaks', 0)

          if cross_tenant > 0:
              print(f'CRITICAL: Cross-tenant interactions detected: {cross_tenant}')
              exit(1)

          if data_leaks > 0:
              print(f'CRITICAL: Data leaks detected: {data_leaks}')
              exit(1)

          print('Security validation passed')
          "

  publish-report:
    name: Publish Report
    runs-on: ubuntu-latest
    needs: [generate-report, validate-report]
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Download reports
        uses: actions/download-artifact@v3
        with:
          name: testbed-reports-${{ github.run_number }}
          path: ./reports

      - name: Create release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: report-${{ github.run_number }}
          release_name: Testbed Report ${{ github.run_number }}
          body: |
            ## Testbed Report ${{ github.run_number }}

            Generated on $(date -u +%Y-%m-%dT%H:%M:%SZ)

            ### Files
            - HTML Report: `testbed_report.html`
            - PDF Report: `testbed_report.pdf`
            - Summary: `report_summary.json`

            ### Key Metrics
            See the attached files for detailed metrics and analysis.

            This report provides comprehensive insights into the Provability Fabric Testbed performance, security, and cost metrics.
          draft: false
          prerelease: false

      - name: Upload release assets
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create-release.outputs.upload_url }}
          asset_path: ./reports/testbed_report.html
          asset_name: testbed_report.html
          asset_content_type: text/html

      - name: Upload PDF asset
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create-release.outputs.upload_url }}
          asset_path: ./reports/testbed_report.pdf
          asset_name: testbed_report.pdf
          asset_content_type: application/pdf

      - name: Upload summary asset
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create-release.outputs.upload_url }}
          asset_path: ./reports/report_summary.json
          asset_name: report_summary.json
          asset_content_type: application/json

      - name: Notify stakeholders
        run: |
          echo "Report published successfully"
          echo "Release URL: ${{ steps.create-release.outputs.html_url }}"
          echo "Report files uploaded as release assets"

volumes:
  prometheus_data:
